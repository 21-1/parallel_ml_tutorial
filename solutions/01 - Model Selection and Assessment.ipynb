{
 "metadata": {
  "name": "01 - Model Selection and Assessment"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model Selection and Assessement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Outline of the session:\n",
      "\n",
      "- Model performance evaluation and detection of overfitting with Cross-Validation\n",
      "- Hyper parameter tuning and model selection with Grid Search\n",
      "- Error analysis with learning curves and the Bias-Variance trade-off\n",
      "- Overfitting via Model Selection and the Development / Evaluation set split"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab as pl\n",
      "import numpy as np\n",
      "pl.gray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Overfitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Overfitting is the problem of learning the training data by heart and being unable to generalize by making correct predictions on new data.\n",
      "\n",
      "Let's load a simple dataset of 8x8 gray level images of handwritten digits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "X, y = digits.data, digits.target\n",
      "print(\"data shape: %r, target shape: %r\" % (X.shape, y.shape))\n",
      "print(\"classes: %r\" % list(np.unique(y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples, n_features = X.shape\n",
      "print(\"n_samples=%d\" % n_samples)\n",
      "print(\"n_features=%d\" % n_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(3):\n",
      "    pl.figure()\n",
      "    pl.imshow(X[i].reshape((8, 8)), interpolation='nearest')\n",
      "    pl.title(\"Target class: %d\" % i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's train a Support Vector Machine naively on this dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "SVC().fit(X, y).score(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Did we really learn a perfect model that can recognize the correct digit class 100% of the time? **Without new data it's impossible to tell.**\n",
      "\n",
      "Let's start again and split the dataset into two random, non overlapping subsets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.25, random_state=0)\n",
      "\n",
      "print(\"train data shape: %r, train target shape: %r\" % (X_train.shape, y_train.shape))\n",
      "print(\"test data shape: %r, test target shape: %r\" % (X_test.shape, y_test.shape))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's retrain a new model on the first subset call the **training set**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc = SVC(kernel='rbf').fit(X_train, y_train)\n",
      "train_score = svc.score(X_train, y_train) \n",
      "train_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now compute the performance of the model on new, held out data from the **test set**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_score = svc.score(X_test, y_test)\n",
      "test_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This score is clearly not as good as expected! The model cannot generalize so well to new, unseen data.\n",
      "\n",
      "- Whenever the **test** data score is **not as good as** the **train** score the model is **overfitting**\n",
      "\n",
      "- Whenever the **train score is not close to 100%** accuracy the model is **underfitting**\n",
      "\n",
      "Ideally **we want to neither overfit nor underfit**: `test_score ~= train_score ~= 1.0`. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The previous example failed to generalized well to test data because we naively used the default parameters of the `SVC` class:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try again with another parameterization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_2 = SVC(kernel='rbf', C=100, gamma=0.001).fit(X_train, y_train)\n",
      "svc_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_2.score(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svc_2.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case the model is almost perfectly able to generalize, at least according to our random train, test split."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- TODO: n_jobs cross validation\n",
      "- TODO: std of test error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import ShuffleSplit\n",
      "\n",
      "cv = ShuffleSplit(n_samples, n_iter=3, test_size=0.1,\n",
      "    random_state=0)\n",
      "\n",
      "for cv_index, (train, test) in enumerate(cv):\n",
      "    print(\"# Cross Validation Iteration #%d\" % cv_index)\n",
      "    print(\"train indices: {0}...\".format(train[:10]))\n",
      "    print(\"test indices: {0}...\".format(test[:10]))\n",
      "    \n",
      "    svc = SVC(kernel=\"rbf\", C=1, gamma=0.001).fit(X[train], y[train])\n",
      "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
      "        svc.score(X[train], y[train]), svc.score(X[test], y[test])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mean_with_ci(scores):\n",
      "    \"\"\"Compute the mean score and standard 95% Confidence Interval\n",
      "\n",
      "    Assume that the distribution of the scores is normal which\n",
      "    is probably not true when the mean is close to an upper or\n",
      "    lower bound.\n",
      "\n",
      "    For better CI evaluation, use scikits.bootstrap.\n",
      "    \"\"\"\n",
      "    scores = np.asarray(scores).ravel()\n",
      "    m, s = np.mean(scores), np.std(scores)\n",
      "    std_error = s / np.sqrt(scores.shape[0])\n",
      "    ci_width = 2 * 1.96 * std_error\n",
      "    return m, ci_width\n",
      "\n",
      "def mean_score(scores):\n",
      "    \"\"\"Print the mean score and standard 95% Confidence Interval\"\"\"\n",
      "    m, ci_width = mean_with_ci(scores)\n",
      "    return (\"Mean score: {0:.3f} [{1:.3f} - {2:.3f}]\").format(\n",
      "        m, m - ci_width / 2, m + ci_width / 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** \n",
      "\n",
      "- Perform 50 iterations of cross validation with randomly sampled folds of 500 training samples and 500 test samples randomly sampled (use `sklearn.cross_validation.ShuffleSplit`).\n",
      "- Try with `SVC(C=1, gamma=0.01)`\n",
      "- Plot the histogram of the test error with 50 bins.\n",
      "- Try to increas the training size\n",
      "- Retry with `SVC(C=10, gamma=0.005)`, then `SVC(C=10, gamma=0.001)` with 500 samples.\n",
      "\n",
      "Hints, type:\n",
      "\n",
      "    from sklearn.cross_validation import ShuffleSplit\n",
      "    ShuffleSplit?  # to read the docstring of the shuffle split\n",
      "    pl.hist?  # to read the docstring of the histogram plot\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = ShuffleSplit(n_samples, n_iter=50, train_size=500, test_size=500,\n",
      "    random_state=0)\n",
      "%time scores = cross_val_score(SVC(C=10, gamma=0.005), X, y, cv=cv)\n",
      "_ = pl.hist(scores, range=(0, 1), bins=50)\n",
      "print(mean_score(scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Plotting Learning Curves for Bias-Variance analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_sizes = np.logspace(2, 3, 3).astype(np.int)\n",
      "train_sizes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Interpreting Learning Curves"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- If the **training set error is high** (e.g. more than 10% misclassification) at the end of the learning curve, the model suffers from **high bias** and is said to **under-fit** the training set.\n",
      "\n",
      "- If the **testing set error is significantly larger than the training set error**, the model suffers from **high variance** and is said to **over-fit** the training set.\n",
      "\n",
      "- Another possible source of high training or testing error is label noise: the data is too noisy and there is nothing few signal learn from it."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What to do against overfitting?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Try to get rid of noisy features using feature selection methods\n",
      "- Try to tune parameters to add more regularization:\n",
      "    - Smaller values of `C` for SVM\n",
      "    - Larger values of `alpha` for penalized linear models\n",
      "    - Restrict to shallower trees (decision stumps) and lower numbers of samples per leafs for tree-based models\n",
      "- Try simpler models such as penalized linear models (e.g. Linear SVM, Logistic Regression, Naive Bayes)\n",
      "- Try the bagging ensemble strategies: average the predictions of independently trained models"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What to do against underfitting?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Give more freedom to the model by relaxing some parameters that act as regularizers:\n",
      "    - Larger values of `C` for SVM\n",
      "    - Smaller values of `alpha` for penalized linear models\n",
      "    - Allow deeper trees and lower numbers of samples per leafs for tree-based models\n",
      "- Try more complex / expressive models:\n",
      "    - Non linear kernel SVMs,\n",
      "    - Ensemble of Decision Trees...)\n",
      "- Construct new features (e.g. bi-gram frequencies for text classifications or K-Means features, feature cross-products or non-linear kernel approximations)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**:\n",
      "\n",
      "TODO: provide the parameters for an underfitting (model, dataset) pair then ask student to plot the learning curves and propose and test solutions \n",
      "\n",
      "TODO: provide the parameters for an overfitting (model, dataset) pair then ask the studen to plot learning curves and propose and test solutions "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parallel Computation of Learning Curves"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "rc = Client()\n",
      "lv = rc.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import namedtuple\n",
      "from collections import defaultdict\n",
      "\n",
      "\n",
      "def compute_evaluation(model, X_train, y_train, X_test, y_test,\n",
      "                       cv_index, params=None):\n",
      "    \"\"\"Function executed on a worker to evaluate a model on a given CV fold\"\"\"\n",
      "    # All module imports should be executed in the worker namespace\n",
      "    from time import time\n",
      "    \n",
      "    # Fit model and measure training time\n",
      "    t0 = time()\n",
      "    model.fit(X_train, y_train)\n",
      "    train_time = time() - t0\n",
      "    \n",
      "    # Compute score on training set\n",
      "    train_score = model.score(X_train, y_train)\n",
      "    \n",
      "    # Compute score on test set\n",
      "    test_score = model.score(X_test, y_test)\n",
      "\n",
      "    # Wrap evaluation results in a simple tuple datastructure\n",
      "    return (test_score, train_score, train_time,\n",
      "            X_train.shape[0], cv_index, params)\n",
      "\n",
      "\n",
      "# Named tuple to collect evaluation results\n",
      "Evaluation = namedtuple('Evaluation', (\n",
      "    'test_score',\n",
      "    'train_score',\n",
      "    'train_time',\n",
      "    'train_size',\n",
      "    'cv_index',\n",
      "    'parameters'))\n",
      "\n",
      "\n",
      "class LearningCurves(object):\n",
      "    \"\"\"Handle async, distributed evaluation of a model learning curves\"\"\"\n",
      "    \n",
      "    def __init__(self, load_balancer):\n",
      "        self._scheduled_tasks = []\n",
      "        self._load_balancer = load_balancer\n",
      "\n",
      "    def evaluate(self, model, X, y, train_sizes=np.linspace(0.1, 0.8, 8),\n",
      "        test_size=0.2, n_iter=5):\n",
      "        \"\"\"Schedule evaluation work in parallel and aggregate results for plotting\"\"\"\n",
      "        \n",
      "        n_samples, n_features = X.shape\n",
      "        self.n_samples = n_samples\n",
      "        \n",
      "        # Abort any other previously scheduled tasks\n",
      "        for task in self._scheduled_tasks:\n",
      "            if not task.ready():\n",
      "                task.abort()\n",
      "\n",
      "        # Schedule a new batch of evalutation tasks\n",
      "        self._scheduled_tasks = []\n",
      "        for train_size in train_sizes:\n",
      "            cv = ShuffleSplit(n_samples, n_iter=n_iter, \n",
      "                              train_size=train_size,\n",
      "                              test_size=test_size)\n",
      "            for cv_index, (train, test) in enumerate(cv):\n",
      "                task = self._load_balancer.apply_async(\n",
      "                    compute_evaluation,\n",
      "                    clf, X[train], y[train], X[test], y[test],\n",
      "                    cv_index)\n",
      "                \n",
      "                self._scheduled_tasks.append(task)\n",
      "        \n",
      "        # Make it possible to chain method calls\n",
      "        return self\n",
      "                \n",
      "    def wait(self):\n",
      "        \"\"\"Wait for completion\"\"\"\n",
      "        for task in self._scheduled_tasks:\n",
      "            task.wait()\n",
      "        \n",
      "        # Make it possible to chain method calls\n",
      "        return self\n",
      "    \n",
      "    def update_summary(self):\n",
      "        \"\"\"Compute summary statistics for all finished tasks\"\"\"\n",
      "        evaluations = [Evaluation(*t.get())\n",
      "                       for t in self._scheduled_tasks if t.ready()]\n",
      "        grouped_evaluations = defaultdict(list)\n",
      "        for ev in evaluations:\n",
      "            # Group evaluations by effective training sizes\n",
      "            grouped_evaluations[ev.train_size].append(ev)\n",
      "        \n",
      "        self.train_sizes = []\n",
      "        self.train_scores, self.train_scores_ci_width = [], []\n",
      "        self.test_scores, self.test_scores_ci_width = [], []\n",
      "        self.train_times, self.train_times_ci_width = [], []\n",
      "    \n",
      "        for size, group in sorted(grouped_evaluations.items()):\n",
      "            self.train_sizes.append(size)\n",
      "    \n",
      "            # Aggregate training scores data\n",
      "            train_mean, train_ci_width = mean_with_ci(\n",
      "                [ev.train_score for ev in group])\n",
      "            self.train_scores.append(train_mean)\n",
      "            self.train_scores_ci_width.append(train_ci_width)\n",
      "    \n",
      "            # Aggregate testing scores data\n",
      "            test_mean, test_ci_width = mean_with_ci(\n",
      "                [ev.test_score for ev in group])\n",
      "            self.test_scores.append(test_mean)\n",
      "            self.test_scores_ci_width.append(test_ci_width)\n",
      "            \n",
      "            # Aggregate training times data\n",
      "            train_time, train_time_ci_width = mean_with_ci(\n",
      "                [ev.train_time for ev in group])\n",
      "            self.train_times.append(train_time)\n",
      "            self.train_times_ci_width.append(train_time_ci_width)\n",
      "            \n",
      "    def __repr__(self):\n",
      "        \"\"\"Display current evaluation statistics\"\"\"\n",
      "        self.update_summary()\n",
      "        if not self.test_scores:\n",
      "            return \"Missing evaluation statistics\"\n",
      "        n_total = len(self._scheduled_tasks)\n",
      "        n_done = len([t for t in self._scheduled_tasks if t.ready()])\n",
      "        last_test = self.test_scores[-1]\n",
      "        last_train = self.train_scores[-1]\n",
      "        last_duration = self.train_times[-1]\n",
      "        overfitting = np.max(last_train - last_test, 0)\n",
      "        underfitting = np.max(1 - last_train, 0)\n",
      "        return (\n",
      "                \"tasks: {n_done}/{n_total}\\n\"\n",
      "                \"last test score: {last_test:.3f}\\n\"\n",
      "                \"last train score: {last_train:.3f}\\n\"\n",
      "                \"last train time: {last_duration:.3f}s\\n\"\n",
      "                \"overfitting: {overfitting:.3f}\\n\"\n",
      "                \"underfitting: {underfitting:.3f}\\n\"\n",
      "        ).format(**locals())\n",
      "\n",
      "\n",
      "def plot_learning_curves(lc):\n",
      "    \"\"\"Interative plot of a learning curve\"\"\"\n",
      "    lc.update_summary()  # ensure that stats are up to date\n",
      "    pl.figure()\n",
      "    if hasattr(lc, 'train_times'):\n",
      "        pl.subplot(211)\n",
      "    pl.errorbar(lc.train_sizes,\n",
      "                lc.train_scores,\n",
      "                lc.train_scores_ci_width,\n",
      "                label='train')\n",
      "    pl.errorbar(lc.train_sizes,\n",
      "                lc.test_scores,\n",
      "                lc.test_scores_ci_width,\n",
      "                label='test')\n",
      "    pl.ylabel('Score')\n",
      "    pl.xlim(0, lc.n_samples)\n",
      "    pl.ylim((None, 1.01))  # The best possible score is 1.0\n",
      "    pl.xticks(())\n",
      "    pl.legend(loc='best')\n",
      "    pl.title('Learning curves for SVM model on digits data')\n",
      "    \n",
      "    if hasattr(lc, 'train_times'):\n",
      "        pl.subplot(212)\n",
      "        pl.errorbar(lc.train_sizes,\n",
      "                    lc.train_times,\n",
      "                    lc.train_times_ci_width)\n",
      "        pl.xlim(0, lc.n_samples)\n",
      "        pl.ylabel('Training time (s)')\n",
      "        pl.xlabel('# training samples')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = SVC(kernel='rbf', C=1, gamma=0.001)\n",
      "lc = LearningCurves(lv)\n",
      "lc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lc.evaluate(clf, X, y, n_iter=50)\n",
      "lc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lv.abort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curves(lc)\n",
      "lc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "plot_learning_curves(lc.wait())\n",
      "lc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model Selection with Grid Search"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: explain"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#help(GridSearchCV)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pprint import pprint\n",
      "params = {\n",
      "    'C': np.logspace(-1, 2, 4),\n",
      "    'gamma': np.logspace(-4, 0, 5)[::-1],\n",
      "}\n",
      "pprint(params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "%time gs = GridSearchCV(SVC(), params, n_jobs=-1).fit(X_train[:500], y_train[:500])\n",
      "gs.best_params_, gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**:\n",
      "\n",
      "- find a set of parameters for an `sklearn.ensemble.ExtraTreesClassifier` on the same dataset to reach at least 94% accuracy on the sample dataset (500 training samples) with a maximum of 100 sub-estimtors (decision trees in this case): `n_estimators=100`\n",
      "- in particular you can grid search good values for `max_depth`, `min_samples_leaf` and optionally `criterion`\n",
      "\n",
      "Hints:\n",
      "\n",
      "Type:\n",
      "\n",
      "    from sklearn.ensemble.ExtraTreesClassifier\n",
      "    ExtraTreesClassifier?  # to read the docstring and know the list of important parameters\n",
      "    print(ExtraTreesClassifier())  # to know the list of default values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "print(ExtraTreesClassifier())\n",
      "# ExtraTreesClassifier?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = {\n",
      "    #'criterion': ['gini', 'entropy'],\n",
      "    'max_depth': [10, 20, None],\n",
      "    'min_samples_leaf': [1, 2, 5],\n",
      "}\n",
      "%time gs = GridSearchCV(ExtraTreesClassifier(n_estimators=100), params, n_jobs=-1).fit(X_train[:500], y_train[:500])\n",
      "gs.best_params_, gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Distributed Model Selection with Grid Search"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import IterGrid\n",
      "\n",
      "params = {\n",
      "    'C': np.logspace(-1, 2, 4),\n",
      "    'gamma': np.logspace(-4, 0, 5)[::-1],\n",
      "}\n",
      "list(IterGrid(params))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DistributedGridSearch(object):\n",
      "    \"\"\"IPython.parallel implementation of grid search\"\"\"\n",
      "\n",
      "    def __init__(self, load_balancer):\n",
      "        self._scheduled_tasks = []\n",
      "        self._load_balancer = load_balancer\n",
      "\n",
      "    def evaluate(self, model, params_grid X, y, n_iter=5):\n",
      "        \"\"\"Schedule evaluation work in parallel and aggregate results for plotting\"\"\"\n",
      "        \n",
      "        # Abort any other previously scheduled tasks\n",
      "        for task in self._scheduled_tasks:\n",
      "            if not task.ready():\n",
      "                task.abort()\n",
      "\n",
      "        # Schedule a new batch of evalutation tasks\n",
      "        self._scheduled_tasks = []\n",
      "        for params in IterGrid(params_grid):\n",
      "            cv = ShuffleSplit(n_samples, n_iter=n_iter, \n",
      "                              train_size=train_size,\n",
      "                              test_size=test_size)\n",
      "            for train, test in cv:\n",
      "                task = self._load_balancer.apply_async(\n",
      "                    compute_evaluation,\n",
      "                    clf, X[train], y[train], X[test], y[test], params=params)\n",
      "                self._scheduled_tasks.append(task)\n",
      "                \n",
      "        # Make it possible to chain method calls\n",
      "        return self\n",
      "                \n",
      "    def wait(self):\n",
      "        \"\"\"Wait for completion\"\"\"\n",
      "        for task in self._scheduled_tasks:\n",
      "            task.wait()\n",
      "        \n",
      "        # Make it possible to chain method calls\n",
      "        return self\n",
      "    \n",
      "    def update_summary(self):\n",
      "        \"\"\"Compute summary statistics for all finished tasks\"\"\"\n",
      "        evaluations = [Evaluation(*t.get())\n",
      "                       for t in self._scheduled_tasks if t.ready()]\n",
      "        grouped_evaluations = defaultdict(list)\n",
      "        for ev in evaluations:\n",
      "            # Group evaluations by effective training sizes\n",
      "            grouped_evaluations[ev.train_size].append(ev)\n",
      "        \n",
      "        self.train_sizes = []\n",
      "        self.train_scores, self.train_scores_ci_width = [], []\n",
      "        self.test_scores, self.test_scores_ci_width = [], []\n",
      "        self.train_times, self.train_times_ci_width = [], []\n",
      "    \n",
      "        for size, group in sorted(grouped_evaluations.items()):\n",
      "            self.train_sizes.append(size)\n",
      "    \n",
      "            # Aggregate training scores data\n",
      "            train_mean, train_ci_width = mean_with_ci(\n",
      "                [ev.train_score for ev in group])\n",
      "            self.train_scores.append(train_mean)\n",
      "            self.train_scores_ci_width.append(train_ci_width)\n",
      "    \n",
      "            # Aggregate testing scores data\n",
      "            test_mean, test_ci_width = mean_with_ci(\n",
      "                [ev.test_score for ev in group])\n",
      "            self.test_scores.append(test_mean)\n",
      "            self.test_scores_ci_width.append(test_ci_width)\n",
      "            \n",
      "            # Aggregate training times data\n",
      "            train_time, train_time_ci_width = mean_with_ci(\n",
      "                [ev.train_time for ev in group])\n",
      "            self.train_times.append(train_time)\n",
      "            self.train_times_ci_width.append(train_time_ci_width)\n",
      "            \n",
      "    def __repr__(self):\n",
      "        \"\"\"Display current evaluation statistics\"\"\"\n",
      "        self.update_summary()\n",
      "        if not self.test_scores:\n",
      "            return \"Missing evaluation statistics\"\n",
      "        n_total = len(self._scheduled_tasks)\n",
      "        n_done = len([t for t in self._scheduled_tasks if t.ready()])\n",
      "        last_test = self.test_scores[-1]\n",
      "        last_train = self.train_scores[-1]\n",
      "        last_duration = self.train_times[-1]\n",
      "        overfitting = np.max(last_train - last_test, 0)\n",
      "        underfitting = np.max(1 - last_train, 0)\n",
      "        return (\n",
      "                \"tasks: {n_done}/{n_total}\\n\"\n",
      "                \"last test score: {last_test:.3f}\\n\"\n",
      "                \"last train score: {last_train:.3f}\\n\"\n",
      "                \"last train time: {last_duration:.3f}s\\n\"\n",
      "                \"overfitting: {overfitting:.3f}\\n\"\n",
      "                \"underfitting: {underfitting:.3f}\\n\"\n",
      "        ).format(**locals())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** learning curves and grid search\n",
      "\n",
      "- combine the previous examples to compute the learning curves data and grid search in parallel\n",
      "- plot the learning curves of the top 3 models (ranked by high test scores and small durations in case of tie)\n",
      "- are the best models overfitting overfitting or underfitting?\n",
      "- would the models benefit from a larger labeled dataset?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Final Model Assessment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Risk of not taking into account the overfitting of the grid search procedure it-self\n",
      "- Development / Evaluation sets split\n",
      "- Development used for Grid Search and training of the model with optimal parameter set\n",
      "- Hold out evaluation set used **only** for estimating the predictive performance of the resulting model\n",
      "- Recommended to use temporal split for the Development / Evaluation split for data points collected over time (for instance  2008-2011 for development, 2012 for evaluation)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: use matplotlib to generate train / validation / test splits where train +  validation is the development set and test is the evaluation set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}