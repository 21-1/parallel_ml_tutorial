{
 "metadata": {
  "name": "04 - Large Scale Text Classification for Sentiment Analysis"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab as pl\n",
      "import numpy as np\n",
      "\n",
      "# Some nice default configuration for plots\n",
      "pl.rcParams['figure.figsize'] = 10, 7.5\n",
      "pl.rcParams['axes.grid'] = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Large Scale Text Classification for Sentiment Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Outline of the Session"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Limitations of the Vocabulary-Based Vectorizer\n",
      "- The **Hashing Trick**\n",
      "- **Online / Streaming** Text Feature Extraction and Classification\n",
      "- **Parallel** Text Feature Extraction and Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scalability Issues"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `sklearn.feature_extraction.text.CountVectorizer` and `sklearn.feature_extraction.text.TfidfVectorizer` classes suffer from a number of scalability issues that all stem from the internal usage of the `vocabulary_` attribute (a Python dictionary) used to map the unicode string feature names to the integer feature indices.\n",
      "\n",
      "The main scalability issues are:\n",
      "\n",
      "- **Memory usage of the text vectorizer**: the all the string representations of the features are loaded in memory\n",
      "- **Parallelization problems for text feature extraction**: the `vocabulary_` would be a shared state: complex synchronization and overhead\n",
      "- **Impossibility to do online or incremental learning**: the `vocabulary_` needs to be learned from the data: its size cannot be known before making one pass over the full dataset\n",
      "    \n",
      "    \n",
      "To better understand the issue let's have a look at how the `vocabulary_` attribute work. At `fit` time the tokens of the corpus are uniquely indentified by a integer index and this mapping stored in the vocabulary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "vectorizer.fit([\n",
      "    \"The cat sat on the mat.\",\n",
      "])\n",
      "vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The vocabulary is used at `transform` time to build the occurence matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.transform([\n",
      "    \"The cat sat on the mat.\",\n",
      "    \"This cat is a nice cat.\",\n",
      "]).toarray()\n",
      "\n",
      "print(len(vectorizer.vocabulary_))\n",
      "print(vectorizer.get_feature_names())\n",
      "print(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's refit with a slightly larget corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer()\n",
      "\n",
      "vectorizer.fit([\n",
      "    \"The cat sat on the mat.\",\n",
      "    \"The quick brown fox jumps over the lazy dog.\",\n",
      "])\n",
      "vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `vocabulary_` is the (logarithmically) growing with the size of the training corpus. Note that we could not have built the vocabularies in parallel on the 2 text documents as they share some words hence would require some kind of shared datastructure or synchronization barrier which is complicated to setup, especially if we want to distribute the processing on a cluster.\n",
      "\n",
      "With this new vocabulary, the dimensionality of the output space is now larger:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.transform([\n",
      "    \"The cat sat on the mat.\",\n",
      "    \"This cat is a nice cat.\",\n",
      "]).toarray()\n",
      "\n",
      "print(len(vectorizer.vocabulary_))\n",
      "print(vectorizer.get_feature_names())\n",
      "print(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Sentiment 140 Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "sentiment140_folder = os.path.join('..', 'datasets', 'sentiment140')\n",
      "training_csv_file = os.path.join(sentiment140_folder, 'training.1600000.processed.noemoticon.csv')\n",
      "testing_csv_file = os.path.join(sentiment140_folder, 'testdata.manual.2009.06.14.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls -lh ../datasets/sentiment140/training.1600000.processed.noemoticon.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's parse the CSV files and load everything in memory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "import csv\n",
      "fieldnames = ('polarity', 'id', 'date', 'query', 'author', 'text')\n",
      "\n",
      "with open(training_csv_file, 'rb') as f:\n",
      "    train_dicts = list(csv.DictReader(f, fieldnames=fieldnames, delimiter=',', quotechar='\"'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(train_dicts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_dicts[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's extract the positive and negative texts and ignore the neutral for now:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "text_train_positive = [d['text'] for d in train_dicts if d['polarity'] == '4']\n",
      "text_train_negative = [d['text'] for d in train_dicts if d['polarity'] == '0']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text_train_positive), len(text_train_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_train_all = text_train_positive + text_train_negative\n",
      "target_train_all = [1] * len(text_train_positive) + [-1] * len(text_train_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "text_train_small, text_validation, target_train_small, target_validation = train_test_split(\n",
      "    text_train_all, target_train_all,\n",
      "    train_size=500000, test_size=100000, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import PassiveAggressiveClassifier\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "p = Pipeline((\n",
      "    ('vec', HashingVectorizer(charset='latin-1')),\n",
      "    ('clf', PassiveAggressiveClassifier(C=1, n_iter=1)),\n",
      "))\n",
      "\n",
      "%time p.fit(text_train_small, target_train_small).score(text_validation, target_validation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(testing_csv_file, 'rb') as f:\n",
      "    test_dicts = list(csv.DictReader(f, fieldnames=fieldnames, delimiter=',', quotechar='\"'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(test_dicts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_dicts[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_test_positive = [d['text'] for d in test_dicts if d['polarity'] == '4']\n",
      "text_test_negative = [d['text'] for d in test_dicts if d['polarity'] == '0']\n",
      "\n",
      "len(text_test_positive), len(text_test_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_test_all = text_test_positive + text_test_negative\n",
      "target_test_all = [1] * len(text_test_positive) + [-1] * len(text_test_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p.score(text_test_all, target_test_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Hashing Trick"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h_vectorizer = HashingVectorizer(charset='latin-1')\n",
      "\n",
      "%time X_train_small = h_vectorizer.fit_transform(text_train_small)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Out-of-Core learning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parallizing Text Feature Extraction and Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}