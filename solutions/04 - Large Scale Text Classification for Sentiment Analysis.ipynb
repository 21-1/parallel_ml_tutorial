{
 "metadata": {
  "name": "04 - Large Scale Text Classification for Sentiment Analysis"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab as pl\n",
      "import numpy as np\n",
      "\n",
      "# Some nice default configuration for plots\n",
      "pl.rcParams['figure.figsize'] = 10, 7.5\n",
      "pl.rcParams['axes.grid'] = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Large Scale Text Classification for Sentiment Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Outline of the Session"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Limitations of the Vocabulary-Based Vectorizer\n",
      "- The **Hashing Trick**\n",
      "- **Online / Streaming** Text Feature Extraction and Classification\n",
      "- **Parallel** Text Feature Extraction and Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scalability Issues"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `sklearn.feature_extraction.text.CountVectorizer` and `sklearn.feature_extraction.text.TfidfVectorizer` classes suffer from a number of scalability issues that all stem from the internal usage of the `vocabulary_` attribute (a Python dictionary) used to map the unicode string feature names to the integer feature indices.\n",
      "\n",
      "The main scalability issues are:\n",
      "\n",
      "- **Memory usage of the text vectorizer**: the all the string representations of the features are loaded in memory\n",
      "- **Parallelization problems for text feature extraction**: the `vocabulary_` would be a shared state: complex synchronization and overhead\n",
      "- **Impossibility to do online or incremental learning**: the `vocabulary_` needs to be learned from the data: its size cannot be known before making one pass over the full dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Sentiment 140 Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "sentiment140_folder = os.path.join('..', 'datasets', 'sentiment140')\n",
      "training_csv_file = os.path.join(sentiment140_folder, 'training.1600000.processed.noemoticon.csv')\n",
      "testing_csv_file = os.path.join(sentiment140_folder, 'testdata.manual.2009.06.14.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls -lh ../datasets/sentiment140/training.1600000.processed.noemoticon.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's parse the CSV files and load everything in memory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "import csv\n",
      "fieldnames = ('polarity', 'id', 'date', 'query', 'author', 'text')\n",
      "\n",
      "with open(training_csv_file, 'rb') as f:\n",
      "    train_dicts = list(csv.DictReader(f, fieldnames=fieldnames, delimiter=',', quotechar='\"'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(train_dicts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_dicts[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's extract the positive and negative texts and ignore the neutral for now:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "text_train_positive = [d['text'] for d in train_dicts if d['polarity'] == '4']\n",
      "text_train_negative = [d['text'] for d in train_dicts if d['polarity'] == '0']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text_train_positive), len(text_train_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_train_all = text_train_positive + text_train_negative\n",
      "target_train_all = [1] * len(train_text_positive) + [-1] * len(train_text_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "text_train_small, text_validation, target_train_small, target_validation = train_test_split(\n",
      "    text_train_all, target_train_all,\n",
      "    train_size=500000, test_size=100000, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import PassiveAggressiveClassifier\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "p = Pipeline((\n",
      "    ('vec', HashingVectorizer(charset='latin-1', ngram_range=(1, 2))),\n",
      "    ('clf', PassiveAggressiveClassifier(C=1, n_iter=1)),\n",
      "))\n",
      "\n",
      "%time p.fit(text_train_small, target_train_small).score(text_validation, target_validation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "with open(testing_csv_file, 'rb') as f:\n",
      "    test_dicts = list(csv.DictReader(f, fieldnames=fieldnames, delimiter=',', quotechar='\"'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(test_dicts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_dicts[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_test_positive = [d['text'] for d in test_dicts if d['polarity'] == '4']\n",
      "text_test_negative = [d['text'] for d in test_dicts if d['polarity'] == '0']\n",
      "\n",
      "len(text_test_positive), len(text_test_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_test_all = text_test_positive + text_test_negative\n",
      "target_test_all = [1] * len(text_test_positive) + [-1] * len(text_test_negative)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p.score(text_test_all, target_test_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Hashing Trick"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Out-of-Core learning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parallizing Text Feature Extraction and Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}